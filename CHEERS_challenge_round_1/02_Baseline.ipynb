{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "after-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "from os.path import join\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "from transformers import BertModel\n",
    "bert_id = \"google/bert_uncased_L-2_H-128_A-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-characteristic",
   "metadata": {},
   "source": [
    "# 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "assured-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_joint = pd.read_hdf(join(\"preprocessed_data\",\"train_joint.h5\"), key=\"s\")\n",
    "validation_join = pd.read_hdf(join(\"preprocessed_data\",\"validation_joint.h5\"), key=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "manual-format",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>sector_ids</th>\n",
       "      <th>sentence_position</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>project_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>url</th>\n",
       "      <th>text_length</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">51657</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.252273</td>\n",
       "      <td>[101, 2047, 4341, 2937, 3360, 2415, 1999, 2474...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>7.699389</td>\n",
       "      <td>2.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>4.127134</td>\n",
       "      <td>[101, 10110, 2003, 2012, 1996, 2415, 1997, 199...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>7.699389</td>\n",
       "      <td>2.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>4.682131</td>\n",
       "      <td>[101, 1996, 2047, 3360, 2415, 2038, 2366, 2062...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>7.699389</td>\n",
       "      <td>2.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>[101, 1996, 4341, 2937, 3360, 2415, 2001, 2764...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>7.699389</td>\n",
       "      <td>2.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>4.204693</td>\n",
       "      <td>[101, 2116, 1997, 2122, 3360, 2272, 2013, 3532...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>7.699389</td>\n",
       "      <td>2.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">34512</th>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>[101, 2174, 1010, 11470, 19621, 2015, 2024, 20...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.068493</td>\n",
       "      <td>4.836282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.812184</td>\n",
       "      <td>5.017280</td>\n",
       "      <td>[101, 1999, 5712, 1010, 2045, 2024, 4311, 1997...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.068493</td>\n",
       "      <td>4.836282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.820282</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>[101, 1996, 9353, 9331, 2015, 2136, 2097, 2562...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.068493</td>\n",
       "      <td>4.836282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.828314</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>[101, 2017, 2064, 2424, 2019, 19184, 1997, 203...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.068493</td>\n",
       "      <td>4.836282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.836282</td>\n",
       "      <td>5.420535</td>\n",
       "      <td>[101, 2340, 9353, 9331, 2015, 11470, 3229, 191...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.068493</td>\n",
       "      <td>4.836282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261981 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    is_relevant sector_ids  sentence_position  \\\n",
       "doc_id sentence_id                                              \n",
       "51657  0                      0         []           0.000000   \n",
       "       1                      0         []           0.693147   \n",
       "       2                      0         []           1.098612   \n",
       "       3                      0         []           1.386294   \n",
       "       4                      0         []           1.609438   \n",
       "...                         ...        ...                ...   \n",
       "34512  121                    0         []           4.804021   \n",
       "       122                    0         []           4.812184   \n",
       "       123                    0         []           4.820282   \n",
       "       124                    0         []           4.828314   \n",
       "       125                    0         []           4.836282   \n",
       "\n",
       "                    sentence_length  \\\n",
       "doc_id sentence_id                    \n",
       "51657  0                   5.252273   \n",
       "       1                   4.127134   \n",
       "       2                   4.682131   \n",
       "       3                   4.875197   \n",
       "       4                   4.204693   \n",
       "...                             ...   \n",
       "34512  121                 4.234107   \n",
       "       122                 5.017280   \n",
       "       123                 4.234107   \n",
       "       124                 4.174387   \n",
       "       125                 5.420535   \n",
       "\n",
       "                                                   tokenized_sentence  \\\n",
       "doc_id sentence_id                                                      \n",
       "51657  0            [101, 2047, 4341, 2937, 3360, 2415, 1999, 2474...   \n",
       "       1            [101, 10110, 2003, 2012, 1996, 2415, 1997, 199...   \n",
       "       2            [101, 1996, 2047, 3360, 2415, 2038, 2366, 2062...   \n",
       "       3            [101, 1996, 4341, 2937, 3360, 2415, 2001, 2764...   \n",
       "       4            [101, 2116, 1997, 2122, 3360, 2272, 2013, 3532...   \n",
       "...                                                               ...   \n",
       "34512  121          [101, 2174, 1010, 11470, 19621, 2015, 2024, 20...   \n",
       "       122          [101, 1999, 5712, 1010, 2045, 2024, 4311, 1997...   \n",
       "       123          [101, 1996, 9353, 9331, 2015, 2136, 2097, 2562...   \n",
       "       124          [101, 2017, 2064, 2424, 2019, 19184, 1997, 203...   \n",
       "       125          [101, 2340, 9353, 9331, 2015, 11470, 3229, 191...   \n",
       "\n",
       "                    project_name  country_code  url  text_length  \\\n",
       "doc_id sentence_id                                                 \n",
       "51657  0                       1             2  139     7.699389   \n",
       "       1                       1             2  139     7.699389   \n",
       "       2                       1             2  139     7.699389   \n",
       "       3                       1             2  139     7.699389   \n",
       "       4                       1             2  139     7.699389   \n",
       "...                          ...           ...  ...          ...   \n",
       "34512  121                     5             1    0    10.068493   \n",
       "       122                     5             1    0    10.068493   \n",
       "       123                     5             1    0    10.068493   \n",
       "       124                     5             1    0    10.068493   \n",
       "       125                     5             1    0    10.068493   \n",
       "\n",
       "                    sentence_count  \n",
       "doc_id sentence_id                  \n",
       "51657  0                  2.944439  \n",
       "       1                  2.944439  \n",
       "       2                  2.944439  \n",
       "       3                  2.944439  \n",
       "       4                  2.944439  \n",
       "...                            ...  \n",
       "34512  121                4.836282  \n",
       "       122                4.836282  \n",
       "       123                4.836282  \n",
       "       124                4.836282  \n",
       "       125                4.836282  \n",
       "\n",
       "[261981 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-motor",
   "metadata": {},
   "source": [
    "# 2 Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "retained-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsRelevantDataset(Dataset):\n",
    "    def __init__(self, joint_dataframe: pd.DataFrame, device=device, dimensions = None):\n",
    "        self.X = joint_dataframe[[\"sentence_position\", \"sentence_length\", \"tokenized_sentence\", \"project_name\", \"country_code\", \"url\", \"text_length\", \"sentence_count\"]].to_numpy()\n",
    "        self.Y = joint_dataframe[\"is_relevant\"].to_numpy()\n",
    "        self.device = device\n",
    "        \n",
    "        if dimensions is None:\n",
    "            self.dimensions = ((1, (4, len(set(self.X[:,3])), len(set(self.X[:,4])), len(set(self.X[:,5])))), 2)\n",
    "        else:\n",
    "            self.dimensions = dimensions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx, x_one_hot = True, x_train_ready = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Note that x_train_ready implies x_one_hot\n",
    "        \"\"\"\n",
    "        x_tmp = self.X[idx]\n",
    "        metric_x = torch.tensor([x_tmp[0], x_tmp[1], x_tmp[6], x_tmp[7]], device=self.device)#numerical features\n",
    "        sentence_x = torch.tensor(x_tmp[2], device=self.device, dtype=torch.long)#bert features\n",
    "        sentence_x = torch.cat((sentence_x, torch.zeros(512 - sentence_x.shape[0], device=self.device, dtype= torch.long)))\n",
    "        \n",
    "        #one hot features:\n",
    "        project_name_x = torch.tensor(x_tmp[3], device=self.device, dtype=torch.long)\n",
    "        country_code_x = torch.tensor(x_tmp[4], device=self.device, dtype=torch.long)\n",
    "        url_x = torch.tensor(x_tmp[5], device=self.device)\n",
    "        \n",
    "        y = torch.tensor(self.Y[idx], device=self.device, dtype=torch.long)\n",
    "\n",
    "        if x_train_ready or x_one_hot:\n",
    "            project_name_x = nn.functional.one_hot(project_name_x, num_classes = self.dimensions[0][1][1])\n",
    "            country_code_x = nn.functional.one_hot(country_code_x, num_classes = self.dimensions[0][1][2])\n",
    "            url_x = nn.functional.one_hot(url_x, num_classes = self.dimensions[0][1][3])\n",
    "        if x_train_ready:\n",
    "            x_other = torch.cat((metric_x, project_name_x, country_code_x, url_x), dim=0)\n",
    "            return (sentence_x, x_other), y\n",
    "        \n",
    "        return (sentence_x, (metric_x, project_name_x, country_code_x, url_x)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "technological-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = IsRelevantDataset(train_joint, device = device)\n",
    "validation_ds = IsRelevantDataset(validation_join, device = device, dimensions = train_ds.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "three-inside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 7.9678e-01,  4.3363e-01,  3.0845e-01,  ..., -1.0185e+00,\n",
       "           4.1972e-01, -1.2215e+00],\n",
       "         [ 1.7186e+00,  2.3990e-01, -1.1979e+00,  ..., -7.5705e-01,\n",
       "           2.4631e-01,  2.9912e-02],\n",
       "         [ 9.1795e-01,  8.0292e-01, -1.1369e+00,  ...,  1.2281e-03,\n",
       "           3.3684e-01, -2.4455e-01],\n",
       "         ...,\n",
       "         [ 5.6881e-01,  6.9844e-01, -8.0349e-01,  ..., -3.5966e-01,\n",
       "           6.5368e-01,  5.5346e-01],\n",
       "         [ 1.0129e+00, -1.6660e-01,  1.7266e-01,  ..., -7.5208e-01,\n",
       "          -1.1192e-01,  2.0395e-01],\n",
       "         [ 3.7733e-01,  9.6487e-02, -7.2973e-01,  ..., -8.6767e-01,\n",
       "          -9.0913e-02,  2.4773e-01]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.2679,  0.5989,  0.4089, -0.1942,  0.1793,  0.3579,  0.6238, -0.5477,\n",
       "         -0.4427,  0.6706,  0.3099,  0.1910,  0.5141, -0.8082,  0.3162,  0.6797,\n",
       "          0.0857,  0.0825, -0.6264,  0.4433,  0.0432, -0.4913, -0.6055,  0.8876,\n",
       "         -0.5522, -0.4214, -0.2152,  0.0151,  0.3245, -0.4773, -0.8568,  0.3034,\n",
       "          0.3273, -0.1744, -0.4612, -0.0861,  0.2031, -0.1015, -0.7012,  0.3528,\n",
       "         -0.5364, -0.2488, -0.8268, -0.8115, -0.1546, -0.6727, -0.5365, -0.6127,\n",
       "          0.5415, -0.3261,  0.8637, -0.2518,  0.3159,  0.1591,  0.1443,  0.4247,\n",
       "          0.4744,  0.1282, -0.7474, -0.3956,  0.5665,  0.4046, -0.0722, -0.2048,\n",
       "         -0.2556, -0.1472,  0.7356, -0.3198,  0.1671,  0.3500, -0.2608,  0.1084,\n",
       "         -0.4429, -0.6883, -0.1748, -0.3079,  0.3051,  0.7081,  0.3017, -0.2059,\n",
       "          0.2185,  0.0088, -0.7897, -0.0131,  0.6514, -0.1536, -0.2822,  0.3653,\n",
       "         -0.1295,  0.6033, -0.5569, -0.7430,  0.0692, -0.4153, -0.1204, -0.5757,\n",
       "          0.1884, -0.6424, -0.7351, -0.3666,  0.1085,  0.3936,  0.2452,  0.0887,\n",
       "         -0.3713, -0.0473,  0.8163, -0.1123, -0.3424, -0.3858,  0.2151, -0.3380,\n",
       "         -0.1057, -0.5723, -0.2141, -0.4296, -0.0694, -0.5383, -0.4033, -0.3964,\n",
       "          0.2157, -0.3595,  0.6477, -0.2654,  0.7137,  0.4776,  0.7646, -0.4430,\n",
       "         -0.1326, -0.5258,  0.2547, -0.4486, -0.2292, -0.1516,  0.8623,  0.5825,\n",
       "         -0.5178,  0.2837,  0.7398, -0.8695,  0.4338, -0.3063,  0.7898,  0.1628,\n",
       "         -0.0577, -0.1603,  0.1133,  0.0700, -0.6972, -0.8454,  0.6365,  0.2852,\n",
       "          0.7938, -0.5578,  0.4943,  0.1024, -0.4693,  0.5292,  0.4603,  0.4583,\n",
       "         -0.8078,  0.3290,  0.0188,  0.2765, -0.2183,  0.0067, -0.1127, -0.2553,\n",
       "          0.4089,  0.5208, -0.1859,  0.0669,  0.1951,  0.7357,  0.5819, -0.0229,\n",
       "          0.0583,  0.1064, -0.6139,  0.2347, -0.5769,  0.8610,  0.4445,  0.0205,\n",
       "         -0.1359, -0.1446,  0.3762,  0.8396,  0.6210,  0.0653,  0.0547,  0.3675,\n",
       "         -0.7679,  0.8305,  0.1131, -0.2340,  0.0906,  0.2238, -0.3215,  0.1464,\n",
       "          0.3834,  0.2678, -0.2033, -0.2680, -0.2727, -0.6055,  0.0688,  0.7445,\n",
       "         -0.4967,  0.0640,  0.6903, -0.3048,  0.2998, -0.1692,  0.5640,  0.4422,\n",
       "         -0.8221,  0.6730, -0.1291, -0.0737, -0.0176, -0.2853, -0.4328,  0.7319,\n",
       "          0.5706, -0.2967,  0.4621,  0.0705,  0.3383,  0.3955,  0.4763,  0.5396,\n",
       "         -0.8002,  0.2386,  0.3282, -0.0059, -0.1399, -0.6993, -0.8221,  0.0185,\n",
       "          0.4872,  0.2091, -0.6764,  0.1503,  0.3358, -0.7157,  0.2641, -0.2323,\n",
       "          0.0168,  0.2640,  0.8044,  0.2458,  0.6089, -0.6959, -0.4333,  0.2288,\n",
       "         -0.0134, -0.6375,  0.7297, -0.1019, -0.0422, -0.3662,  0.0344, -0.0988,\n",
       "         -0.5648,  0.1074,  0.5532, -0.3516,  0.6632, -0.9130,  0.9128,  0.5160,\n",
       "          0.3481,  0.5704,  0.3415, -0.4455,  0.3855,  0.7473,  0.5405,  0.4393,\n",
       "          0.4294,  0.3959,  0.0280,  0.8762, -0.3551, -0.4459,  0.2616,  0.2743,\n",
       "         -0.6237,  0.0211, -0.3756, -0.8448, -0.4255,  0.1372,  0.6764, -0.3289,\n",
       "         -0.8473,  0.2108, -0.6715,  0.2317, -0.6903,  0.5570, -0.1668, -0.3722,\n",
       "         -0.3946, -0.4131,  0.4292, -0.7207,  0.4908,  0.6873, -0.2594, -0.8437,\n",
       "          0.0104, -0.2579,  0.8305, -0.2495, -0.2805, -0.6659, -0.3589, -0.1824,\n",
       "          0.1739, -0.3018,  0.4694,  0.9603,  0.0530,  0.2074, -0.4642,  0.8319,\n",
       "         -0.0171, -0.8102,  0.3756,  0.1756, -0.8279, -0.6990,  0.3025,  0.1074,\n",
       "         -0.1648,  0.4094,  0.5433, -0.6945, -0.0198, -0.1475,  0.3996,  0.7911,\n",
       "         -0.8552, -0.7163, -0.3213,  0.3269, -0.1252, -0.6193, -0.1441,  0.5047,\n",
       "          0.3541, -0.0889, -0.5801, -0.3298,  0.4704,  0.1377,  0.8096,  0.4820,\n",
       "          0.2548,  0.0092, -0.8622,  0.3206, -0.3133, -0.3249, -0.5257, -0.1939,\n",
       "         -0.3997,  0.1825, -0.7730, -0.3193,  0.0429, -0.6725,  0.1000,  0.2909,\n",
       "         -0.1745, -0.7068,  0.5428, -0.4704,  0.7313,  0.1490, -0.3764, -0.4642,\n",
       "         -0.1461,  0.0633, -0.4267,  0.2207, -0.2112, -0.3610, -0.1760,  0.1928,\n",
       "          0.4175, -0.1611,  0.1190,  0.5998,  0.5085,  0.0196, -0.6807,  0.1779,\n",
       "          0.6346,  0.2700, -0.1431,  0.2754,  0.6526,  0.0748, -0.2160,  0.0500,\n",
       "         -0.7487,  0.6561, -0.1712,  0.5189, -0.1159,  0.0096, -0.2664,  0.1273,\n",
       "         -0.0168,  0.1003, -0.0288, -0.2050,  0.1553,  0.2033, -0.0862,  0.2247,\n",
       "          0.4442, -0.4667, -0.6215,  0.5056,  0.1196,  0.6598,  0.3365, -0.6331,\n",
       "         -0.4754, -0.3042, -0.1868, -0.5964,  0.3222,  0.0877,  0.2727,  0.0868,\n",
       "          0.3945,  0.4048, -0.5380,  0.1788,  0.1628,  0.2665, -0.0325, -0.5466,\n",
       "         -0.7398,  0.0358,  0.2947,  0.8038, -0.7524, -0.7302,  0.1667,  0.0442,\n",
       "         -0.8130,  0.7344,  0.2956, -0.4007, -0.1627,  0.1993,  0.0845,  0.7708,\n",
       "          0.2154,  0.2484,  0.3562,  0.0205, -0.0271, -0.3563,  0.7290, -0.6480,\n",
       "          0.2693,  0.8562, -0.5393,  0.3495,  0.4940, -0.2689,  0.0707,  0.7153,\n",
       "          0.7004,  0.5008,  0.4907,  0.6761,  0.0307,  0.8386, -0.6195, -0.5815,\n",
       "         -0.1644, -0.3617, -0.0183, -0.6315, -0.8657, -0.0688,  0.3055, -0.1166,\n",
       "         -0.5329, -0.4918, -0.4874, -0.1390, -0.0262, -0.4091, -0.2145, -0.8776,\n",
       "          0.0686,  0.1797,  0.6806,  0.5349,  0.0698, -0.6448,  0.1611, -0.1484,\n",
       "         -0.0636,  0.4594, -0.3367, -0.1129, -0.4865, -0.1649,  0.6953,  0.2994,\n",
       "         -0.4162, -0.5444, -0.6019,  0.0385, -0.1554,  0.4183, -0.1068,  0.2150,\n",
       "          0.5834,  0.4631, -0.1530,  0.3089, -0.2615, -0.0749,  0.3333, -0.4998,\n",
       "         -0.0585, -0.0042,  0.1441,  0.3444, -0.3857, -0.4979,  0.3937,  0.3798,\n",
       "          0.0775, -0.7676, -0.3569, -0.6617, -0.7397, -0.2065, -0.5753, -0.1773,\n",
       "          0.7051,  0.5780, -0.4303, -0.1260,  0.5416,  0.2495,  0.1786, -0.0280,\n",
       "          0.5790, -0.2956,  0.8586, -0.0242,  0.4918, -0.0234, -0.7202,  0.1116,\n",
       "          0.5766,  0.2864, -0.2328, -0.6876,  0.6375,  0.0063,  0.0473, -0.0012,\n",
       "         -0.8037, -0.6894,  0.7163,  0.4684, -0.0894,  0.1971, -0.3678,  0.6890,\n",
       "          0.2065, -0.4417, -0.8957, -0.6922, -0.8018,  0.3962, -0.0178, -0.2650,\n",
       "         -0.0403, -0.5873, -0.0787,  0.3749, -0.4579, -0.0474,  0.3895, -0.5597,\n",
       "         -0.5089, -0.5703, -0.6703, -0.8048, -0.6615, -0.3741,  0.2817,  0.2853,\n",
       "          0.1366,  0.5283,  0.1420,  0.4538, -0.1329,  0.2957, -0.4990, -0.3280,\n",
       "          0.0645, -0.5006,  0.6342, -0.4116, -0.3248,  0.0186,  0.0063,  0.4662,\n",
       "          0.3197, -0.0577, -0.3768,  0.9110, -0.4521, -0.1793, -0.3611,  0.1055,\n",
       "         -0.2133,  0.7330, -0.2377, -0.7781,  0.2338,  0.5587, -0.3849, -0.6753,\n",
       "          0.5784,  0.0297,  0.1829, -0.7048, -0.7258, -0.5889,  0.5821, -0.6254,\n",
       "          0.3822,  0.5676,  0.8812,  0.2854, -0.4072,  0.0159,  0.6867, -0.0616,\n",
       "         -0.5118, -0.5189,  0.8181,  0.8197, -0.6500, -0.3960, -0.5731, -0.1738,\n",
       "         -0.5000, -0.2965, -0.4076,  0.8832,  0.6730,  0.1529,  0.5782,  0.4252,\n",
       "         -0.1197, -0.0244,  0.3143,  0.2592,  0.9501, -0.4181,  0.3777, -0.6768,\n",
       "          0.2547, -0.4422, -0.1672, -0.5252, -0.2593,  0.6964,  0.1593,  0.3919,\n",
       "         -0.6881,  0.5084,  0.8367, -0.3919, -0.2021, -0.1374, -0.6560,  0.0325,\n",
       "          0.3509, -0.1811, -0.1687, -0.3118,  0.5544, -0.3967, -0.5019, -0.3127,\n",
       "         -0.3617, -0.4720,  0.5054,  0.3910,  0.0034,  0.7234,  0.2831,  0.7147,\n",
       "          0.3079, -0.2763,  0.1397, -0.0479, -0.1250,  0.9028,  0.2519,  0.4483,\n",
       "          0.1254,  0.2619,  0.4798,  0.1653, -0.0514, -0.4609,  0.0821, -0.5762,\n",
       "         -0.1898, -0.6613, -0.4023, -0.4407,  0.4719,  0.5404, -0.1446,  0.8937,\n",
       "          0.7827, -0.5326,  0.2783, -0.1989,  0.0816,  0.3916, -0.2062,  0.4517,\n",
       "         -0.2772,  0.3376,  0.0897, -0.1609,  0.4222, -0.8694,  0.3885, -0.5173,\n",
       "         -0.3311,  0.8479, -0.6685, -0.5506, -0.4536,  0.3471, -0.3799,  0.4858,\n",
       "          0.0929,  0.4968, -0.4940,  0.2417,  0.6760, -0.0391,  0.5210,  0.6176]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem = torch.unsqueeze(train_ds.__getitem__(1)[0][0], 0)\n",
    "bert_model(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-assignment",
   "metadata": {},
   "source": [
    "# 4 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fleet-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsRelevantNet(nn.Module):\n",
    "    def __init__(self, bert: BertModel, input_size, output_size):\n",
    "        super(IsRelevantNet, self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            #nn.BatchNorm1d(bert.config.hidden_size + input_size),#just a feeling this might be nice\n",
    "            nn.Linear(bert.config.hidden_size + input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_bert = x[0]\n",
    "        x_other = x[1]\n",
    "        y_bert = self.bert(x[0])[\"last_hidden_state\"][:,0] #all batches but only clf output\n",
    "        \n",
    "        x = torch.cat((y_bert, x_other), dim=1)#dim=1 is feature dimensions (0 is batch dim)\n",
    "        \n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-border",
   "metadata": {},
   "source": [
    "# 5 Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vanilla-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, optimizer, loss, loader, tracker=None):\n",
    "    model.train()\n",
    "    \n",
    "    for x, y in loader:\n",
    "        y_hat = model(x)\n",
    "        l = loss(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"\\r {l.item()}\",end=\"\")\n",
    "        if tracker:\n",
    "            tracker.next_train(l)\n",
    "    \n",
    "    if tracker:\n",
    "        tracker.submit_train()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, metric, loader, tracker=None):\n",
    "    model.eval()\n",
    "    \n",
    "    for x,y in loader:\n",
    "        y_hat = model(x)\n",
    "        l = metric(y_hat, y)\n",
    "        if tracker:\n",
    "            tracker.next_eval(l)\n",
    "            \n",
    "    if tracker:\n",
    "        tracker.submit_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imposed-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size=16\n",
    "epochs = 10\n",
    "bert_hidden = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nominated-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706415979ae349f2a888214fe2db355b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/382 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd536ecb69004197b59c104b8c19293b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters (including bert): 4528002\n",
      "Number of trainable parameters (excluding bert): 142082\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds,batch_size  = batch_size, shuffle=True)\n",
    "validation_dl = DataLoader(train_ds,batch_size  = 64, shuffle=False)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model = IsRelevantNet(BertModel.from_pretrained(bert_id).to(device), sum(train_ds.dimensions[0][1]), train_ds.dimensions[1]).to(device)\n",
    "#Should not train bert (for now)\n",
    "model.bert.train(False)\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = False\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "print(f\"Number of parameters (including bert): {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters (excluding bert): {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "successful-analyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.27797731757164244"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-393378ba83a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-aafaa14b4f35>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(model, optimizer, loss, loader, tracker)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\main\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\main\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\main\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\main\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-91ed00960fc9>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx, x_one_hot, x_train_ready)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mproject_name_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject_name_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mcountry_code_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountry_code_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0murl_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx_train_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mx_other\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_name_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountry_code_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_epoch in range(1, epochs+1):\n",
    "    update(model, optimizer, loss, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-selection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-table",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
