{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "after-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "from os.path import join\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "from urllib.parse import urlparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboard import notebook\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "from transformers import BertModel\n",
    "bert_id = \"google/bert_uncased_L-2_H-128_A-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-characteristic",
   "metadata": {},
   "source": [
    "# 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assured-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_joint = pd.read_hdf(join(\"preprocessed_data\",\"train_joint.h5\"), key=\"s\")\n",
    "validation_join = pd.read_hdf(join(\"preprocessed_data\",\"validation_joint.h5\"), key=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manual-format",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    is_relevant sector_ids  sentence_position  \\\n",
       "doc_id sentence_id                                              \n",
       "51657  0                      0         []           0.000000   \n",
       "       1                      0         []           0.693147   \n",
       "       2                      0         []           1.098612   \n",
       "       3                      0         []           1.386294   \n",
       "       4                      0         []           1.609438   \n",
       "...                         ...        ...                ...   \n",
       "34512  121                    0         []           4.804021   \n",
       "       122                    0         []           4.812184   \n",
       "       123                    0         []           4.820282   \n",
       "       124                    0         []           4.828314   \n",
       "       125                    0         []           4.836282   \n",
       "\n",
       "                    sentence_length  \\\n",
       "doc_id sentence_id                    \n",
       "51657  0                   5.252273   \n",
       "       1                   4.127134   \n",
       "       2                   4.682131   \n",
       "       3                   4.875197   \n",
       "       4                   4.204693   \n",
       "...                             ...   \n",
       "34512  121                 4.234107   \n",
       "       122                 5.017280   \n",
       "       123                 4.234107   \n",
       "       124                 4.174387   \n",
       "       125                 5.420535   \n",
       "\n",
       "                                                   tokenized_sentence  \\\n",
       "doc_id sentence_id                                                      \n",
       "51657  0            [101, 2047, 4341, 2937, 3360, 2415, 1999, 2474...   \n",
       "       1            [101, 10110, 2003, 2012, 1996, 2415, 1997, 199...   \n",
       "       2            [101, 1996, 2047, 3360, 2415, 2038, 2366, 2062...   \n",
       "       3            [101, 1996, 4341, 2937, 3360, 2415, 2001, 2764...   \n",
       "       4            [101, 2116, 1997, 2122, 3360, 2272, 2013, 3532...   \n",
       "...                                                               ...   \n",
       "34512  121          [101, 2174, 1010, 11470, 19621, 2015, 2024, 20...   \n",
       "       122          [101, 1999, 5712, 1010, 2045, 2024, 4311, 1997...   \n",
       "       123          [101, 1996, 9353, 9331, 2015, 2136, 2097, 2562...   \n",
       "       124          [101, 2017, 2064, 2424, 2019, 19184, 1997, 203...   \n",
       "       125          [101, 2340, 9353, 9331, 2015, 11470, 3229, 191...   \n",
       "\n",
       "                    project_name  country_code  url  text_length  \\\n",
       "doc_id sentence_id                                                 \n",
       "51657  0                       1             2  139     7.699389   \n",
       "       1                       1             2  139     7.699389   \n",
       "       2                       1             2  139     7.699389   \n",
       "       3                       1             2  139     7.699389   \n",
       "       4                       1             2  139     7.699389   \n",
       "...                          ...           ...  ...          ...   \n",
       "34512  121                     5             1    0    10.068493   \n",
       "       122                     5             1    0    10.068493   \n",
       "       123                     5             1    0    10.068493   \n",
       "       124                     5             1    0    10.068493   \n",
       "       125                     5             1    0    10.068493   \n",
       "\n",
       "                    sentence_count  \n",
       "doc_id sentence_id                  \n",
       "51657  0                  2.944439  \n",
       "       1                  2.944439  \n",
       "       2                  2.944439  \n",
       "       3                  2.944439  \n",
       "       4                  2.944439  \n",
       "...                            ...  \n",
       "34512  121                4.836282  \n",
       "       122                4.836282  \n",
       "       123                4.836282  \n",
       "       124                4.836282  \n",
       "       125                4.836282  \n",
       "\n",
       "[261981 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>is_relevant</th>\n      <th>sector_ids</th>\n      <th>sentence_position</th>\n      <th>sentence_length</th>\n      <th>tokenized_sentence</th>\n      <th>project_name</th>\n      <th>country_code</th>\n      <th>url</th>\n      <th>text_length</th>\n      <th>sentence_count</th>\n    </tr>\n    <tr>\n      <th>doc_id</th>\n      <th>sentence_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">51657</th>\n      <th>0</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>0.000000</td>\n      <td>5.252273</td>\n      <td>[101, 2047, 4341, 2937, 3360, 2415, 1999, 2474...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>139</td>\n      <td>7.699389</td>\n      <td>2.944439</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>0.693147</td>\n      <td>4.127134</td>\n      <td>[101, 10110, 2003, 2012, 1996, 2415, 1997, 199...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>139</td>\n      <td>7.699389</td>\n      <td>2.944439</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>1.098612</td>\n      <td>4.682131</td>\n      <td>[101, 1996, 2047, 3360, 2415, 2038, 2366, 2062...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>139</td>\n      <td>7.699389</td>\n      <td>2.944439</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>1.386294</td>\n      <td>4.875197</td>\n      <td>[101, 1996, 4341, 2937, 3360, 2415, 2001, 2764...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>139</td>\n      <td>7.699389</td>\n      <td>2.944439</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>1.609438</td>\n      <td>4.204693</td>\n      <td>[101, 2116, 1997, 2122, 3360, 2272, 2013, 3532...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>139</td>\n      <td>7.699389</td>\n      <td>2.944439</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">34512</th>\n      <th>121</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>4.804021</td>\n      <td>4.234107</td>\n      <td>[101, 2174, 1010, 11470, 19621, 2015, 2024, 20...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10.068493</td>\n      <td>4.836282</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>4.812184</td>\n      <td>5.017280</td>\n      <td>[101, 1999, 5712, 1010, 2045, 2024, 4311, 1997...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10.068493</td>\n      <td>4.836282</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>4.820282</td>\n      <td>4.234107</td>\n      <td>[101, 1996, 9353, 9331, 2015, 2136, 2097, 2562...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10.068493</td>\n      <td>4.836282</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>4.828314</td>\n      <td>4.174387</td>\n      <td>[101, 2017, 2064, 2424, 2019, 19184, 1997, 203...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10.068493</td>\n      <td>4.836282</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>0</td>\n      <td>[]</td>\n      <td>4.836282</td>\n      <td>5.420535</td>\n      <td>[101, 2340, 9353, 9331, 2015, 11470, 3229, 191...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10.068493</td>\n      <td>4.836282</td>\n    </tr>\n  </tbody>\n</table>\n<p>261981 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-motor",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retained-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsRelevantDataset(Dataset):\n",
    "    def __init__(self, joint_dataframe: pd.DataFrame, device=device, dimensions = None):\n",
    "        self.X = joint_dataframe[[\"sentence_position\", \"sentence_length\", \"tokenized_sentence\", \"project_name\", \"country_code\", \"url\", \"text_length\", \"sentence_count\"]].to_numpy()\n",
    "        self.Y = joint_dataframe[\"is_relevant\"].to_numpy()\n",
    "        self.device = device\n",
    "        \n",
    "        if dimensions is None:\n",
    "            self.dimensions = ((1, (4, len(set(self.X[:,3])), len(set(self.X[:,4])), len(set(self.X[:,5])))), 2)\n",
    "        else:\n",
    "            self.dimensions = dimensions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx, x_one_hot = True, x_train_ready = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Note that x_train_ready implies x_one_hot\n",
    "        \"\"\"\n",
    "        x_tmp = self.X[idx]\n",
    "        metric_x = torch.tensor([x_tmp[0], x_tmp[1], x_tmp[6], x_tmp[7]], device=self.device)#numerical features\n",
    "        sentence_x = torch.tensor(x_tmp[2], device=self.device, dtype=torch.long)#bert features\n",
    "        sentence_x = torch.cat((sentence_x, torch.zeros(512 - sentence_x.shape[0], device=self.device, dtype= torch.long)))\n",
    "        \n",
    "        #one hot features:\n",
    "        project_name_x = torch.tensor(x_tmp[3], device=self.device, dtype=torch.long)\n",
    "        country_code_x = torch.tensor(x_tmp[4], device=self.device, dtype=torch.long)\n",
    "        url_x = torch.tensor(x_tmp[5], device=self.device)\n",
    "        \n",
    "        y = torch.tensor(self.Y[idx], device=self.device, dtype=torch.long)\n",
    "\n",
    "        if x_train_ready or x_one_hot:\n",
    "            project_name_x = nn.functional.one_hot(project_name_x, num_classes = self.dimensions[0][1][1])\n",
    "            country_code_x = nn.functional.one_hot(country_code_x, num_classes = self.dimensions[0][1][2])\n",
    "            url_x = nn.functional.one_hot(url_x, num_classes = self.dimensions[0][1][3])\n",
    "        if x_train_ready:\n",
    "            x_other = torch.cat((metric_x, project_name_x, country_code_x, url_x), dim=0)\n",
    "            return (sentence_x, x_other), y\n",
    "        \n",
    "        return (sentence_x, (metric_x, project_name_x, country_code_x, url_x)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "technological-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = IsRelevantDataset(train_joint, device = device)\n",
    "validation_ds = IsRelevantDataset(validation_join, device = device, dimensions = train_ds.dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-assignment",
   "metadata": {},
   "source": [
    "# 4 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fleet-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsRelevantNet(nn.Module):\n",
    "    def __init__(self, bert: BertModel, input_size, output_size):\n",
    "        super(IsRelevantNet, self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.linear_after_bert = nn.Linear(bert.config.hidden_size, 256)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            #nn.BatchNorm1d(bert.config.hidden_size + input_size),#just a feeling this might be nice\n",
    "            nn.Linear(256 + input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_bert = x[0]\n",
    "        x_other = x[1]\n",
    "        y_bert = self.bert(x[0])[\"last_hidden_state\"][:,0] #all batches but only clf output\n",
    "        y_bert = self.linear_after_bert(y_bert)\n",
    "        x = torch.cat((y_bert, x_other), dim=1)#dim=1 is feature dimensions (0 is batch dim)\n",
    "        \n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-border",
   "metadata": {},
   "source": [
    "# 5 Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "vanilla-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, optimizer, loss, loader, logger , i):\n",
    "    model.train()\n",
    "    for (x, y) in tqdm(loader):\n",
    "        i+=1\n",
    "        y_hat = model(x)\n",
    "        l = loss(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if logger:\n",
    "            logger.add_scalar(\"loss\",l.detach().cpu(),i)\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, metric, loader, logger = None):\n",
    "    model.eval()\n",
    "    \n",
    "    for x,y in loader:\n",
    "        y_hat = model(x)\n",
    "        l = metric(y_hat, y)\n",
    "        if logger:\n",
    "            logger.next_eval(l)\n",
    "            \n",
    "    if logger:\n",
    "        logger.submit_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "imposed-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "batch_size=16\n",
    "epochs = 10\n",
    "bert_hidden = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "nominated-helping",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of parameters (including bert): 5132098\nNumber of trainable parameters (excluding bert): 5132098\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds,batch_size  = batch_size, shuffle=True)\n",
    "validation_dl = DataLoader(train_ds, batch_size  = 64, shuffle=False)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model = IsRelevantNet(BertModel.from_pretrained(bert_id).to(device), sum(train_ds.dimensions[0][1]), train_ds.dimensions[1]).to(device)\n",
    "#Should not train bert (for now)\n",
    "#model.bert.train(False)\n",
    "#for p in model.bert.parameters():\n",
    "#    p.requires_grad = False\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "print(f\"Number of parameters (including bert): {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters (excluding bert): {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16374\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selecting TensorBoard with logdir log6 (started 0:00:04 ago; port 6006, pid 16864).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-fbf88cc77b6d09f1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-fbf88cc77b6d09f1\");\n          const url = new URL(\"http://localhost\");\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "base_logdir = \"logs\"\n",
    "logdir = join(base_logdir, \"log6\")\n",
    "tensorboard_port = 6006\n",
    "sw = SummaryWriter(logdir)\n",
    "\n",
    "!start tensorboard --reload_multifile true --port $tensorboard_port --logdir $logdir\n",
    "time.sleep(8)\n",
    "notebook.display(port=tensorboard_port, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "successful-analyst",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/16374 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "594a7f8789424d65925b99278e447ca7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/16374 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "130eada51650462a9addf050033bbc16"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/16374 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "499cda2db7784064978e5ad5531cf8b3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/16374 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15d4a33672dd43de8469a829fe389154"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/16374 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "670ce719f5034a9c95fb8eac443e5fae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-a146ccc74c89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-124977d197f7>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(model, optimizer, loss, loader, logger, i)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\main\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\main\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_epoch in range(1, epochs+1):\n",
    "    update(model, optimizer, loss, train_dl, sw, (n_epoch-1)*len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-selection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-table",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd04c83c5b9b5d2d50ddcda366d25ccf04f0f3b336554a780e447a2eb46bdf20827",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}