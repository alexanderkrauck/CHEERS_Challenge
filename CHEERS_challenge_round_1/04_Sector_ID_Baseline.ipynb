{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4519c7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.architectures import SectorModuleV00\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything, LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from transformers import BertModel\n",
    "from utils.data import RelevantDataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from transformers import BertModel\n",
    "bert_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "\n",
    "notebook_datetime = datetime.now()\n",
    "seed_everything(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad943541",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66e266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        target_mode: str = \"isrelevant\",\n",
    "        device: str = \"cpu\",\n",
    "        dimensions: tuple = None,\n",
    "        load_only_relevant: bool = False\n",
    "    ):\n",
    "        \"\"\"Constructor Function\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : str\n",
    "            Decides which dataset will be loaded. Can be either \"train\", \"test\" or \"val\".\n",
    "        target_mode : str\n",
    "            Decides which target is returned in the __getitem__ function.\n",
    "            Can be either \"isrelevant\", \"sentencetype\" or \"both\".TODO:!!!!\n",
    "        device : str\n",
    "            Decides on which device the torch tensors will be returned.\n",
    "        dimensions : tuple\n",
    "            The dimensions to use for returning one hot encodings.\n",
    "        load_only_relevant : bool\n",
    "            If true the Dataset will only contain samples with the \"relevant\" target equal True.\n",
    "        \"\"\" \n",
    "\n",
    "        if dataset == \"train\":\n",
    "            joint_dataframe = pd.read_hdf(\"./preprocessed_data/train_joint.h5\", key=\"s\")\n",
    "        if dataset == \"val\":\n",
    "            joint_dataframe = pd.read_hdf(\"./preprocessed_data/validation_joint.h5\", key=\"s\")\n",
    "            if not dimensions:\n",
    "                raise TypeError(\"Dimensions attribute is required for dataset type \\\"validation\\\".\")\n",
    "        if dataset == \"test\":\n",
    "            joint_dataframe = pd.read_hdf(\"./preprocessed_data/test_joint.h5\", key=\"s\")\n",
    "            if not dimensions:\n",
    "                raise TypeError(\"Dimensions attribute is required for dataset type \\\"test\\\".\")\n",
    "        if load_only_relevant:\n",
    "            joint_dataframe = joint_dataframe[joint_dataframe[\"is_relevant\"] == True]\n",
    "\n",
    "          \n",
    "        if target_mode == \"isrelevant\":\n",
    "            self.X = joint_dataframe[[\"sentence_position\",\n",
    "                                      \"sentence_length\",\n",
    "                                      \"tokenized_sentence\", \n",
    "                                      \"project_name\", \n",
    "                                      \"country_code\",\n",
    "                                      \"url\",\n",
    "                                      \"text_length\",\n",
    "                                      \"sentence_count\"]].to_numpy()\n",
    "            self.Y = joint_dataframe[\"is_relevant\"].to_numpy()\n",
    "            if dimensions is None:\n",
    "                self.dimensions = ((1, (4, \n",
    "                                        len(set(self.X[:,3])), \n",
    "                                        len(set(self.X[:,4])), \n",
    "                                        len(set(self.X[:,5]))+1)),\n",
    "                                   1)\n",
    "            else:\n",
    "                self.dimensions = dimensions\n",
    "\n",
    "        if target_mode == \"sentencetype\":\n",
    "            self.X = joint_dataframe[joint_dataframe[\"is_relevant\"] == 1][[\"sentence_position\",\n",
    "                                                                           \"sentence_length\",\n",
    "                                                                           \"tokenized_sentence\",\n",
    "                                                                           \"project_name\", \n",
    "                                                                           \"country_code\", \n",
    "                                                                           \"url\", \n",
    "                                                                           \"text_length\",\n",
    "                                                                           \"sentence_count\"]].to_numpy()\n",
    "            joint_dataframe.loc[joint_dataframe[\"sector_ids\"].apply(len) == 0, \"sector_ids\"] = 11\n",
    "            joint_dataframe[\"sector_ids\"] = joint_dataframe[\"sector_ids\"].apply(lambda x: x[0] if type(x) != int else x)\n",
    "            self.Y = joint_dataframe[joint_dataframe[\"is_relevant\"] == 1][\"sector_ids\"].to_numpy()\n",
    "            if dimensions is None:\n",
    "                self.dimensions = ((1, (4, \n",
    "                                        len(set(joint_dataframe.to_numpy()[:,5])), \n",
    "                                        len(set(joint_dataframe.to_numpy()[:,6])),\n",
    "                                        len(set(joint_dataframe.to_numpy()[:,7]))+1\n",
    "                                       )\n",
    "                                   ),\n",
    "                                   len(set(self.Y[:])))\n",
    "            else:\n",
    "                self.dimensions = dimensions\n",
    "            \n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx, x_one_hot = True, x_train_ready = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Note that x_train_ready implies x_one_hot\n",
    "        \"\"\"\n",
    "        x_tmp = self.X[idx]\n",
    "        metric_x = torch.tensor([x_tmp[0], x_tmp[1], x_tmp[6], x_tmp[7]], device=self.device)#numerical features\n",
    "        sentence_x = torch.tensor(x_tmp[2], device=self.device, dtype=torch.long)#bert features\n",
    "        sentence_x = torch.cat((sentence_x, \n",
    "                                torch.zeros(512 - sentence_x.shape[0],\n",
    "                                            device=self.device, \n",
    "                                            dtype= torch.long)))\n",
    "        \n",
    "        #one hot features:\n",
    "        project_name_x = torch.tensor(x_tmp[3], device=self.device, dtype=torch.long)\n",
    "        country_code_x = torch.tensor(x_tmp[4], device=self.device, dtype=torch.long)\n",
    "        url_x = torch.tensor(x_tmp[5], device=self.device)\n",
    "        \n",
    "        y = torch.tensor(self.Y[idx], device=self.device, dtype=torch.long)\n",
    "\n",
    "        if x_train_ready or x_one_hot:\n",
    "            project_name_x = nn.functional.one_hot(project_name_x, num_classes = self.dimensions[0][1][1])\n",
    "            country_code_x = nn.functional.one_hot(country_code_x, num_classes = self.dimensions[0][1][2])\n",
    "            url_x = nn.functional.one_hot(url_x, num_classes = self.dimensions[0][1][3])\n",
    "        if x_train_ready:\n",
    "            x_other = torch.cat((metric_x, project_name_x, country_code_x, url_x), dim=0)\n",
    "            return (sentence_x, x_other), y\n",
    "        \n",
    "        return (sentence_x, (metric_x, project_name_x, country_code_x, url_x)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "447ea8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RelevantDataset(dataset=\"train\", \n",
    "                           target_mode=\"sentencetype\"\n",
    "                          )\n",
    "valid_ds = RelevantDataset(dataset=\"val\", \n",
    "                           target_mode=\"sentencetype\",\n",
    "                           dimensions = train_ds.dimensions\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7493d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(train_ds, open(\"train_ds_04_large\", \"wb\"))\n",
    "pickle.dump(valid_ds, open(\"valid_ds_04_large\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af68c0",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48febb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 1e-4\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10312b",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20304ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,batch_size  = batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(train_ds, batch_size  = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cd590",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab01768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SectorModuleV00(\n",
    "    bert = BertModel.from_pretrained(bert_id).to(device),\n",
    "    input_size = sum(train_ds.dimensions[0][1]), \n",
    "    output_size = train_ds.dimensions[1],\n",
    "    start_lr=start_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf96d8",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e6d2f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs/SectorModuleV00/2021-04-28T12-06-09\n"
     ]
    }
   ],
   "source": [
    "architecture_name = model.__class__.__name__\n",
    "logdir = join(\"logs\", architecture_name, notebook_datetime.strftime(\"%Y-%m-%dT%H-%M-%S\"))\n",
    "print(f\"Logging to {logdir}\")\n",
    "Path(logdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(logdir, name=\"\", version=\"\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=logdir,\n",
    "    verbose=True,\n",
    "    save_last=True,\n",
    "    save_top_k=-1, #save all\n",
    "    mode=\"min\",\n",
    "    filename='-{epoch:02d}-{val_loss:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b266ce",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb182594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "#     gpus=1,\n",
    "    gpus=0,\n",
    "#     precision=16, \n",
    "    logger=tb_logger, \n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e631f08",
   "metadata": {},
   "source": [
    "## Executing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3ba3209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | bert              | BertModel  | 4.4 M \n",
      "1 | linear_after_bert | Linear     | 33.0 K\n",
      "2 | feed_forward      | Sequential | 714 K \n",
      "-------------------------------------------------\n",
      "5.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 M     Total params\n",
      "20.535    Total estimated model params size (MB)\n",
      "/home/loerinczy/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loerinczy/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6932aa6c0c734e939c6a3ebf2174636f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loerinczy/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Saving latest checkpoint...\n",
      "Epoch 0, global step 4: val_loss reached 2.72789 (best 2.72789), saving model to \"/home/loerinczy/Desktop/CHEERS_Challenge/CHEERS_challenge_round_1/logs/SectorModuleV00/2021-04-28T12-06-09/-epoch=00-val_loss=2.73.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e561a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
