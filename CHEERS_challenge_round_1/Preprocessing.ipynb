{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences_path, documents_path, train=True):\n",
    "    \"\"\"preprocessor function\"\"\"\n",
    "    \n",
    "    # load data    \n",
    "    sents = pd.read_csv(sentences_path, converters={'sector_ids': literal_eval})\n",
    "    docs = pd.read_csv(documents_path)\n",
    "    docs = docs.copy().set_index(\"doc_id\")\n",
    "    sents[\"position\"] = sents[\"sentence_id\"].apply(lambda x: np.log(x + 1))\n",
    "    sents = sents.copy().copy().set_index([\"doc_id\",\"sentence_id\"])\n",
    "    \n",
    "    # change nominal features to indices\n",
    "    docs[\"doc_url\"].fillna(\"\",inplace=True)\n",
    "    project_name_mapping = dict((o,idx) for idx, o in enumerate(set(docs[\"project_name\"])))\n",
    "    country_code_mapping = dict((o,idx) for idx, o in enumerate(set(docs[\"country_code\"])))\n",
    "    url_set = set(docs[\"doc_url\"].apply(lambda x: urlparse(x).netloc))\n",
    "    document_url_mapping = dict((o,idx) for idx, o in enumerate(url_set))\n",
    "    docs.replace(project_name_mapping, inplace=True)\n",
    "    docs.replace(country_code_mapping, inplace=True)\n",
    "    docs[\"url\"] = docs[\"doc_url\"].apply(lambda x: urlparse(x).netloc).replace(document_url_mapping)\n",
    "    if train == False:\n",
    "        for item in docs.iterrows():\n",
    "            if urlparse(item[1][\"doc_url\"]).netloc not in url_set:\n",
    "                docs.loc[item[0], \"url\"] = len(url_set)\n",
    "    \n",
    "    \n",
    "    # feature exctractor\n",
    "    docs[\"text_lenght\"] = docs[\"doc_text\"].apply(len).apply(np.log)\n",
    "    docs[\"sentence_count\"] = sents.groupby(level=\"doc_id\").size().apply(np.log)\n",
    "    sents[\"sentence_lenght\"] = sents[\"sentence_text\"].apply(len).apply(np.log)\n",
    "    \n",
    "    # tokenization\n",
    "    sents[\"tokenized_text\"] = sents[\"sentence_text\"].apply(lambda x:\\\n",
    "                                tokenizer(x, max_length=512, truncation=\"longest_first\")[\"input_ids\"])\n",
    "    sents.drop(\"sentence_text\", axis=\"columns\", inplace= True)\n",
    "    \n",
    "    # remove unnecessary features\n",
    "    docs.drop(\"lang_code\",axis=\"columns\", inplace = True)\n",
    "    docs.drop(\"doc_text\",axis=\"columns\", inplace = True)\n",
    "    docs.drop(\"doc_url\",axis=\"columns\", inplace = True)\n",
    "    \n",
    "    # join the tables\n",
    "    joint = sents.join(docs, on=\"doc_id\")\n",
    "    \n",
    "    return joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
