{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything, LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from transformers import BertModel\n",
    "from utils.data import RelevantDataset\n",
    "import importlib\n",
    "\n",
    "#Notebook Parameters\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "bert_id = \"google/bert_uncased_L-2_H-128_A-2\" #small bert (only 2 layers I think)\n",
    "#bert_id = \"bert-base-uncased\" # 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n",
    "\n",
    "noetbook_datetime = datetime.now()\n",
    "\n",
    "seed_everything(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.architectures import RelevantModuleV03 as RelevantModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RelevantDataset(dataset=\"train\", target_mode=\"isrelevant\", device=device, move_to_tensor=False)\n",
    "validation_ds = RelevantDataset(dataset=\"val\", device = device, target_mode=\"isrelevant\", dimensions = train_ds.dimensions, move_to_tensor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 1e-4\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs/RelevantModuleV03/2021-06-13T10-58-32\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = RelevantModule(\n",
    "    bert = BertModel.from_pretrained(bert_id).to(device),\n",
    "    input_size = sum(train_ds.dimensions[0][1]), \n",
    "    output_size = train_ds.dimensions[1],\n",
    "    start_lr=start_lr,\n",
    "    prior = train_ds.prior\n",
    ")\n",
    "\n",
    "\n",
    "#monitoring\n",
    "architecture_name = model.__class__.__name__\n",
    "logdir = join(\"logs\", architecture_name, noetbook_datetime.strftime(\"%Y-%m-%dT%H-%M-%S\"))\n",
    "print(f\"Logging to {logdir}\")\n",
    "Path(logdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(logdir, name=\"\", version=\"\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='f1_support',\n",
    "    dirpath=logdir,\n",
    "    verbose=True,\n",
    "    save_last=True,\n",
    "    save_top_k=-1, #save all\n",
    "    mode=\"min\",\n",
    "    filename='-{epoch:02d}-{val_loss:.2f}'\n",
    ")\n",
    "\n",
    "#Dataloaders\n",
    "train_dl = DataLoader(train_ds,batch_size  = batch_size, shuffle=True, num_workers=4)\n",
    "validation_dl = DataLoader(validation_ds, batch_size  = 64, shuffle=False, num_workers=4)\n",
    "\n",
    "# training\n",
    "trainer = Trainer(\n",
    "    gpus=1, \n",
    "    precision=16, \n",
    "    logger=tb_logger, \n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | bert              | BertModel  | 4.4 M \n",
      "1 | linear_after_bert | Linear     | 33.0 K\n",
      "2 | feed_forward      | Sequential | 714 K \n",
      "-------------------------------------------------\n",
      "5.1 M     Trainable params\n",
      "2         Non-trainable params\n",
      "5.1 M     Total params\n",
      "20.532    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/miniconda3/envs/cheers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/alexander/miniconda3/envs/cheers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Global seed set to 1337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6ec56025044566b7ed2b2bddc9df35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dl, validation_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Visit: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#restoring-training-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils.data\n",
    "#utils.data.preprocess(\"data_round_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor([1,0]), num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.precision_recall_fscore_support(np.random.randint(0,2, size=10), np.random.randint(0,2, size=10), average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "00c91dd9e034b3689ce0ac933cca3a4f1db3a4d7109cfd0482a1b2662818e442"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
